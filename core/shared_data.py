# -*- coding: utf-8 -*-
"""
å…±äº«æ•°æ®ç®¡ç†å™¨ - çŸ¥è¯†åº“ä½¿ç”¨JSONå­˜å‚¨ï¼Œé›†æˆå‘é‡æ£€ç´¢

ä¼˜åŒ–å†…å®¹ (v2.3.0):
- æ·»åŠ ç¼“å­˜æœºåˆ¶å‡å°‘é‡å¤æ£€ç´¢
- é›†æˆæ€§èƒ½ç›‘æ§
- æ·»åŠ çŸ¥è¯†åº“å»é‡æ£€æµ‹
"""


from typing import List, Optional, Tuple, Dict, Callable
from dataclasses import dataclass, field, asdict
from datetime import datetime
import json
import os
import logging
import re

from core.config import Config


logger = logging.getLogger(__name__)


BASE_SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå•†å®¢æœåŠ©æ‰‹ï¼Œè´Ÿè´£è§£ç­”ç”¨æˆ·å…³äºå•†å“ã€è®¢å•ã€ç‰©æµã€é€€æ¢è´§ç­‰é—®é¢˜ã€‚è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­æ°”å›å¤ï¼Œå›ç­”è¦ç®€æ´æœ‰å¸®åŠ©ã€‚"


def truncate_text(text: Optional[str], max_chars: int) -> str:
    if not text:
        return ""
    if max_chars is None or max_chars <= 0:
        return str(text)
    text = str(text)
    if len(text) <= max_chars:
        return text
    return text[:max_chars]


def trim_history(history: Optional[list], max_messages: int, max_chars: int) -> list:
    if not history:
        return []

    normalized = []
    for msg in history:
        if not isinstance(msg, dict):
            continue
        role = msg.get("role")
        content = msg.get("content")
        if role not in ("user", "assistant") or not content:
            continue
        normalized.append({"role": role, "content": str(content)})

    if not normalized:
        return []

    if max_messages is not None and max_messages > 0:
        normalized = normalized[-max_messages:]

    if max_chars is None or max_chars <= 0:
        return normalized

    picked_reversed = []
    total = 0
    for msg in reversed(normalized):
        content = msg.get("content", "")
        if not content:
            continue
        next_total = total + len(content)
        if next_total > max_chars:
            if not picked_reversed:
                picked_reversed.append({
                    "role": msg.get("role", "user"),
                    "content": truncate_text(content, max_chars),
                })
            break
        picked_reversed.append(msg)
        total = next_total

    return list(reversed(picked_reversed))


def build_system_prompt(context_text: Optional[str]) -> str:
    if context_text:
        return (
            f"{BASE_SYSTEM_PROMPT}\n\n"
            "ä»¥ä¸‹æ˜¯ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼Œè¯·å‚è€ƒè¿™äº›ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼š\n\n"
            "---çŸ¥è¯†åº“å†…å®¹å¼€å§‹---\n"
            f"{context_text}\n"
            "---çŸ¥è¯†åº“å†…å®¹ç»“æŸ---\n\n"
            "è¯·åŸºäºä¸Šè¿°çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœçŸ¥è¯†åº“å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œå¯ä»¥é€‚å½“è¡¥å……ï¼Œä½†è¦ä¿æŒä¸“ä¸šå’Œå‡†ç¡®ã€‚"
        )
    return BASE_SYSTEM_PROMPT


def build_messages(system_prompt: str, user_message: str, history: Optional[list] = None) -> list:
    messages = [{"role": "system", "content": system_prompt}]
    if history:
        for msg in history:
            role = msg.get("role")
            content = msg.get("content")
            if role in ("user", "assistant") and content:
                messages.append({"role": role, "content": str(content)})
    messages.append({"role": "user", "content": user_message})
    return messages


def format_prompt_preview(messages: list) -> str:
    parts = []
    for m in messages:
        role = m.get("role", "")
        content = m.get("content", "")
        if role == "system":
            parts.append(f"ã€System Messageã€‘\n{content}")
        elif role == "user":
            parts.append(f"ã€User Messageã€‘\n{content}")
        elif role == "assistant":
            parts.append(f"ã€Assistant Messageã€‘\n{content}")
    return "\n\n".join(parts)


@dataclass
class KnowledgeItem:
    """çŸ¥è¯†æ¡ç›®"""
    id: str
    question: str
    answer: str
    keywords: List[str] = field(default_factory=list)
    category: str = "é€šç”¨"
    score: float = 1.0
    
    def to_dict(self) -> dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: dict) -> 'KnowledgeItem':
        return cls(
            id=data.get("id", ""),
            question=data.get("question", ""),
            answer=data.get("answer", ""),
            keywords=data.get("keywords", []),
            category=data.get("category", "é€šç”¨"),
            score=data.get("score", 1.0)
        )


@dataclass
class ProductItem:
    """å•†å“ä¿¡æ¯"""
    id: str                                          # å•†å“IDï¼Œå¦‚ P001
    name: str                                        # å•†å“åç§°
    price: float                                     # ä»·æ ¼
    category: str                                    # å•†å“åˆ†ç±»
    description: str                                 # å•†å“æè¿°
    specifications: Dict[str, str] = field(default_factory=dict)  # è§„æ ¼å‚æ•°
    stock: int = 0                                   # åº“å­˜æ•°é‡
    keywords: List[str] = field(default_factory=list)  # å…³é”®è¯
    
    def to_dict(self) -> dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: dict) -> 'ProductItem':
        return cls(
            id=data.get("id", ""),
            name=data.get("name", ""),
            price=data.get("price", 0.0),
            category=data.get("category", ""),
            description=data.get("description", ""),
            specifications=data.get("specifications", {}),
            stock=data.get("stock", 0),
            keywords=data.get("keywords", [])
        )
    
    def generate_knowledge_items(self) -> List[dict]:
        """ç”Ÿæˆå¯¹åº”çš„çŸ¥è¯†æ¡ç›®æ•°æ®"""
        items = []
        
        # æ„å»ºè§„æ ¼æ–‡æœ¬
        spec_text = ""
        if self.specifications:
            spec_lines = [f"  - {k}: {v}" for k, v in self.specifications.items()]
            spec_text = "\n".join(spec_lines)
        
        # 1. å•†å“åŸºæœ¬ä¿¡æ¯é—®ç­”
        answer = f"ã€{self.name}ã€‘\n"
        answer += f"ğŸ’° ä»·æ ¼ï¼šÂ¥{self.price:.2f}\n"
        answer += f"ğŸ“¦ åº“å­˜ï¼š{'æœ‰è´§' if self.stock > 0 else 'æš‚æ—¶ç¼ºè´§'}ï¼ˆ{self.stock}ä»¶ï¼‰\n"
        answer += f"ğŸ“ åˆ†ç±»ï¼š{self.category}\n"
        if spec_text:
            answer += f"ğŸ“‹ è§„æ ¼ï¼š\n{spec_text}\n"
        answer += f"\nğŸ“ å•†å“æè¿°ï¼š\n{self.description}"
        
        items.append({
            "question": f"{self.name}æ€ä¹ˆæ ·ï¼Ÿ",
            "answer": answer,
            "keywords": self.keywords + [self.name, self.category],
            "category": "å•†å“ä¿¡æ¯"
        })
        
        # 2. ä»·æ ¼æŸ¥è¯¢
        items.append({
            "question": f"{self.name}å¤šå°‘é’±ï¼Ÿ",
            "answer": f"{self.name}çš„ä»·æ ¼æ˜¯ Â¥{self.price:.2f}ã€‚{'ç›®å‰æœ‰è´§' if self.stock > 0 else 'ç›®å‰æš‚æ—¶ç¼ºè´§'}ã€‚",
            "keywords": [self.name, "ä»·æ ¼", "å¤šå°‘é’±"],
            "category": "å•†å“ä¿¡æ¯"
        })
        
        # 3. è§„æ ¼æŸ¥è¯¢ï¼ˆå¦‚æœæœ‰è§„æ ¼ï¼‰
        if self.specifications:
            spec_answer = f"{self.name}çš„è§„æ ¼å‚æ•°å¦‚ä¸‹ï¼š\n{spec_text}"
            items.append({
                "question": f"{self.name}æœ‰ä»€ä¹ˆè§„æ ¼/é…ç½®ï¼Ÿ",
                "answer": spec_answer,
                "keywords": [self.name, "è§„æ ¼", "é…ç½®", "å‚æ•°"],
                "category": "å•†å“ä¿¡æ¯"
            })
        
        # 4. åº“å­˜æŸ¥è¯¢
        stock_status = "æœ‰è´§" if self.stock > 0 else "æš‚æ—¶ç¼ºè´§"
        stock_answer = f"{self.name}ç›®å‰{stock_status}ï¼Œåº“å­˜æ•°é‡ï¼š{self.stock}ä»¶ã€‚"
        if self.stock == 0:
            stock_answer += "\næ‚¨å¯ä»¥ç‚¹å‡»'åˆ°è´§é€šçŸ¥'ï¼Œå•†å“è¡¥è´§åæˆ‘ä»¬ä¼šç¬¬ä¸€æ—¶é—´é€šçŸ¥æ‚¨ã€‚"
        items.append({
            "question": f"{self.name}æœ‰è´§å—ï¼Ÿ",
            "answer": stock_answer,
            "keywords": [self.name, "åº“å­˜", "æœ‰è´§", "ç¼ºè´§"],
            "category": "å•†å“ä¿¡æ¯"
        })
        
        return items

class RAGSearchResult:
    """RAGæœç´¢ç»“æœï¼Œç”¨äºè¿½æº¯"""
    def __init__(self):
        self.query = ""
        self.rewritten_query = ""
        self.retrieved_items: List[Tuple[KnowledgeItem, float]] = []
        self.context_text = ""
        self.confidence = 0.0
        self.search_method = "vector"  # "vector" or "keyword"
        self.final_prompt = ""  # æœ€ç»ˆå‘é€ç»™LLMçš„å®Œæ•´æç¤ºè¯
    
    def to_dict(self) -> dict:
        return {
            "query": self.query,
            "rewritten_query": self.rewritten_query,
            "retrieved_items": [
                {"id": item.id, "question": item.question, "score": score}
                for item, score in self.retrieved_items
            ],
            "context_text": self.context_text,
            "confidence": self.confidence,
            "search_method": self.search_method,
            "final_prompt": self.final_prompt
        }


class KnowledgeStore:
    """çŸ¥è¯†åº“å­˜å‚¨ - JSONæ–‡ä»¶æŒä¹…åŒ– + å‘é‡æ£€ç´¢ + å€’æ’ç´¢å¼•"""
    _cache_mtime: float | None = None
    _cache_raw_items: list[dict] | None = None

    def __init__(self):
        self.items: List[KnowledgeItem] = []
        self.config = Config()
        self._data_file = self._get_data_file()
        self._last_search_result: Optional[RAGSearchResult] = None
        self._last_vector_index_error: Optional[dict] = None
        self._last_chunk_map: Dict[str, List[str]] = {}
        self._embedding_client = None
        self._vector_store = None
        
        # å€’æ’ç´¢å¼•ï¼ˆå…³é”®è¯ -> çŸ¥è¯†æ¡ç›®IDåˆ—è¡¨ï¼‰
        self._inverted_index: Dict[str, List[str]] = {}
        self._index_built = False
        
        # æ€§èƒ½ç›‘æ§
        self._perf_monitor = None
        
        self._load_from_file()
    
    def _get_perf_monitor(self):
        """å»¶è¿ŸåŠ è½½æ€§èƒ½ç›‘æ§å™¨"""
        if self._perf_monitor is None:
            try:
                from core.performance import PerformanceMonitor
                self._perf_monitor = PerformanceMonitor()
            except Exception:
                pass
        return self._perf_monitor

    @property
    def last_vector_index_error(self) -> Optional[dict]:
        return self._last_vector_index_error
    
    def _get_embedding_client(self):
        """å»¶è¿ŸåŠ è½½Embeddingå®¢æˆ·ç«¯"""
        if self._embedding_client is None:
            try:
                from core.embedding import EmbeddingClient
                self._embedding_client = EmbeddingClient()
            except Exception as e:
                logger.exception("åŠ è½½Embeddingå®¢æˆ·ç«¯å¤±è´¥")
        return self._embedding_client
    
    def _get_vector_store(self):
        """å»¶è¿ŸåŠ è½½å‘é‡å­˜å‚¨"""
        if self._vector_store is None:
            try:
                from core.vector_store import VectorStore
                self._vector_store = VectorStore()
            except Exception as e:
                logger.exception("åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥")
        return self._vector_store
    
    def _get_data_file(self) -> str:
        """è·å–æ•°æ®æ–‡ä»¶è·¯å¾„"""
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        data_dir = os.path.join(base_dir, "data")
        os.makedirs(data_dir, exist_ok=True)
        return os.path.join(data_dir, "knowledge_base.json")
    
    def _load_from_file(self):
        """ä»JSONæ–‡ä»¶åŠ è½½çŸ¥è¯†åº“"""
        if os.path.exists(self._data_file):
            try:
                mtime = os.path.getmtime(self._data_file)
                if (
                    self.__class__._cache_mtime == mtime
                    and isinstance(self.__class__._cache_raw_items, list)
                ):
                    self.items = [KnowledgeItem.from_dict(item) for item in self.__class__._cache_raw_items]
                    return
                with open(self._data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    raw_items = data.get("items", []) if isinstance(data, dict) else []
                    raw_items = raw_items if isinstance(raw_items, list) else []
                    self.items = [KnowledgeItem.from_dict(item) for item in raw_items]
                    self.__class__._cache_mtime = mtime
                    self.__class__._cache_raw_items = raw_items
                    logger.info("å·²åŠ è½½ %s æ¡çŸ¥è¯†", len(self.items))
                    
                # æ„å»ºå€’æ’ç´¢å¼•
                self._build_inverted_index()
            except Exception as e:
                logger.exception("åŠ è½½çŸ¥è¯†åº“å¤±è´¥")
                self._load_default_knowledge()
        else:
            self._load_default_knowledge()
            self._save_to_file()
    
    def _save_to_file(self):
        """ä¿å­˜çŸ¥è¯†åº“åˆ°JSONæ–‡ä»¶ï¼ˆå¸¦æ–‡ä»¶é”ï¼‰"""
        try:
            from core.file_lock import FileLock
            
            data = {
                "items": [item.to_dict() for item in self.items],
                "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # ä½¿ç”¨æ–‡ä»¶é”ä¿æŠ¤å†™å…¥
            lock = FileLock(self._data_file, timeout=5.0)
            with lock:
                with open(self._data_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            
            try:
                self.__class__._cache_mtime = os.path.getmtime(self._data_file)
                self.__class__._cache_raw_items = data.get("items", [])
            except Exception:
                self.__class__._cache_mtime = None
                self.__class__._cache_raw_items = None
            logger.info("çŸ¥è¯†åº“å·²ä¿å­˜ï¼Œå…± %s æ¡", len(self.items))
        except TimeoutError:
            logger.error("ä¿å­˜çŸ¥è¯†åº“å¤±è´¥ï¼šæ— æ³•è·å–æ–‡ä»¶é”")
        except Exception as e:
            logger.exception("ä¿å­˜çŸ¥è¯†åº“å¤±è´¥")
    
    def _load_default_knowledge(self):
        """åŠ è½½é»˜è®¤çŸ¥è¯†åº“"""
        default_items = [
            KnowledgeItem(
                id="K001",
                question="å¦‚ä½•ç”³è¯·é€€è´§é€€æ¬¾ï¼Ÿ",
                answer='æ‚¨å¯ä»¥åœ¨æ”¶åˆ°å•†å“å7å¤©å†…ç”³è¯·æ— ç†ç”±é€€è´§ã€‚è¯·è¿›å…¥"æˆ‘çš„è®¢å•"ï¼Œæ‰¾åˆ°å¯¹åº”è®¢å•ç‚¹å‡»"ç”³è¯·é€€è´§"æŒ‰é’®ã€‚ç¡®ä¿å•†å“å®Œå¥½ã€ä¸å½±å“äºŒæ¬¡é”€å”®ã€‚é€€æ¬¾å°†åœ¨1-3ä¸ªå·¥ä½œæ—¥å†…åŸè·¯è¿”å›ã€‚',
                keywords=["é€€è´§", "é€€æ¬¾", "é€€æ¢è´§"],
                category="å”®åæ”¿ç­–"
            ),
            KnowledgeItem(
                id="K002",
                question="æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™å‘è´§ï¼Ÿ",
                answer="æˆ‘ä»¬ä¼šåœ¨ä¸‹å•å24å°æ—¶å†…å‘è´§ï¼ˆèŠ‚å‡æ—¥é¡ºå»¶ï¼‰ã€‚å‘è´§åæ‚¨ä¼šæ”¶åˆ°çŸ­ä¿¡é€šçŸ¥ï¼Œä¹Ÿå¯ä»¥åœ¨è®¢å•è¯¦æƒ…ä¸­æŸ¥çœ‹ç‰©æµå•å·ã€‚",
                keywords=["å‘è´§", "è®¢å•", "ç‰©æµ"],
                category="ç‰©æµé…é€"
            ),
            KnowledgeItem(
                id="K003",
                question="ç‰©æµä¿¡æ¯åœ¨å“ªé‡ŒæŸ¥çœ‹ï¼Ÿ",
                answer="æ‚¨å¯ä»¥åœ¨è®¢å•è¯¦æƒ…é¡µæŸ¥çœ‹ç‰©æµä¿¡æ¯ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ™®é€šå¿«é€’3-5å¤©åˆ°è¾¾ï¼ŒåŠ æ€¥å¿«é€’1-2å¤©åˆ°è¾¾ã€‚å¦‚æœç‰©æµä¿¡æ¯é•¿æ—¶é—´æœªæ›´æ–°ï¼Œå¯èƒ½æ˜¯å¿«é€’å…¬å¸æš‚æœªæ‰«æã€‚",
                keywords=["ç‰©æµ", "å¿«é€’", "é…é€"],
                category="ç‰©æµé…é€"
            ),
            KnowledgeItem(
                id="K004",
                question="æœ‰ä»€ä¹ˆä¼˜æƒ æ´»åŠ¨å—ï¼Ÿ",
                answer="ç›®å‰æˆ‘ä»¬æœ‰ä»¥ä¸‹ä¼˜æƒ æ´»åŠ¨ï¼š\n1. æ–°ç”¨æˆ·é¦–å•ç«‹å‡10å…ƒ\n2. æ»¡200å‡30\n3. éƒ¨åˆ†å•†å“é™æ—¶æŠ˜æ‰£\n\næ‚¨å¯ä»¥åœ¨é¦–é¡µæŸ¥çœ‹æ›´å¤šä¼˜æƒ ä¿¡æ¯ã€‚",
                keywords=["ä¼˜æƒ ", "æŠ˜æ‰£", "æ´»åŠ¨", "ä¿ƒé”€"],
                category="ä¿ƒé”€æ´»åŠ¨"
            ),
            KnowledgeItem(
                id="K005",
                question="å•†å“å°ºç æ€ä¹ˆé€‰æ‹©ï¼Ÿ",
                answer="å…³äºå°ºç é€‰æ‹©ï¼Œå»ºè®®æ‚¨å‚è€ƒå•†å“è¯¦æƒ…é¡µçš„å°ºç è¡¨ã€‚å¦‚æœæ‚¨å¹³æ—¶ç©¿Mç ï¼Œå¯ä»¥å‚è€ƒè¡¨ä¸­Mç å¯¹åº”çš„å…·ä½“å°ºå¯¸ï¼Œä¸æ‚¨çš„å®é™…æµ‹é‡å°ºå¯¸å¯¹æ¯”é€‰æ‹©ã€‚",
                keywords=["å°ºç ", "å°ºå¯¸", "å¤§å°"],
                category="å•†å“å’¨è¯¢"
            ),
            KnowledgeItem(
                id="K006",
                question="æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ",
                answer="æˆ‘ä»¬æ”¯æŒå¤šç§æ”¯ä»˜æ–¹å¼ï¼šæ”¯ä»˜å®ã€å¾®ä¿¡æ”¯ä»˜ã€é“¶è”æ”¯ä»˜ã€ä¿¡ç”¨å¡ç­‰ã€‚æ”¯ä»˜è¿‡ç¨‹é‡‡ç”¨åŠ å¯†ä¼ è¾“ï¼Œè¯·æ”¾å¿ƒä½¿ç”¨ã€‚",
                keywords=["æ”¯ä»˜", "ä»˜æ¬¾", "æ”¯ä»˜å®", "å¾®ä¿¡"],
                category="æ”¯ä»˜é—®é¢˜"
            ),
            KnowledgeItem(
                id="K007",
                question="å¦‚ä½•è”ç³»äººå·¥å®¢æœï¼Ÿ",
                answer='æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»äººå·¥å®¢æœï¼š\n1. ç‚¹å‡»é¡µé¢å³ä¸‹è§’"è½¬äººå·¥"æŒ‰é’®\n2. æ‹¨æ‰“å®¢æœçƒ­çº¿ï¼š400-XXX-XXXX\n3. åœ¨APPå†…é€‰æ‹©"åœ¨çº¿å®¢æœ"\n\næœåŠ¡æ—¶é—´ï¼š9:00-21:00',
                keywords=["äººå·¥", "å®¢æœ", "è”ç³»"],
                category="æœåŠ¡å’¨è¯¢"
            ),
            KnowledgeItem(
                id="K008",
                question="å•†å“è´¨é‡æœ‰ä¿éšœå—ï¼Ÿ",
                answer="æˆ‘ä»¬æ‰€æœ‰å•†å“éƒ½ç»è¿‡ä¸¥æ ¼è´¨é‡æ£€æµ‹ã€‚å¦‚æœæ‚¨æ”¶åˆ°çš„å•†å“å­˜åœ¨è´¨é‡é—®é¢˜ï¼Œè¯·åœ¨æ”¶è´§å48å°æ—¶å†…æ‹ç…§åé¦ˆï¼Œæˆ‘ä»¬å°†ä¸ºæ‚¨å®‰æ’æ¢è´§æˆ–é€€æ¬¾ã€‚",
                keywords=["è´¨é‡", "ä¿éšœ", "æ­£å“"],
                category="å•†å“å’¨è¯¢"
            ),
        ]
        self.items = default_items
        # æ„å»ºå€’æ’ç´¢å¼•
        self._build_inverted_index()
    
    def _build_inverted_index(self):
        """æ„å»ºå€’æ’ç´¢å¼•ä»¥åŠ é€Ÿå…³é”®è¯æ£€ç´¢"""
        self._inverted_index.clear()
        
        for item in self.items:
            # ç´¢å¼•å…³é”®è¯
            for keyword in (item.keywords or []):
                keyword = (keyword or "").strip().lower()
                if keyword:
                    if keyword not in self._inverted_index:
                        self._inverted_index[keyword] = []
                    if item.id not in self._inverted_index[keyword]:
                        self._inverted_index[keyword].append(item.id)
            
            # ç´¢å¼•åˆ†ç±»
            category = (item.category or "").strip().lower()
            if category:
                if category not in self._inverted_index:
                    self._inverted_index[category] = []
                if item.id not in self._inverted_index[category]:
                    self._inverted_index[category].append(item.id)
            
            # æå–é—®é¢˜ä¸­çš„å…³é”®è¯ï¼ˆç®€å•åˆ†è¯ï¼‰
            tokens = self._extract_tokens(item.question)
            for token in tokens[:10]:  # é™åˆ¶æ¯ä¸ªé—®é¢˜æœ€å¤š10ä¸ªtoken
                token = token.lower()
                if token not in self._inverted_index:
                    self._inverted_index[token] = []
                if item.id not in self._inverted_index[token]:
                    self._inverted_index[token].append(item.id)
        
        self._index_built = True
        logger.debug("å€’æ’ç´¢å¼•å·²æ„å»ºï¼Œå…± %s ä¸ªè¯æ¡", len(self._inverted_index))
    
    def _update_inverted_index(self, item: KnowledgeItem, remove: bool = False):
        """å¢é‡æ›´æ–°å€’æ’ç´¢å¼•"""
        if not self._index_built:
            self._build_inverted_index()
            return
        
        # æ”¶é›†è¯¥æ¡ç›®çš„æ‰€æœ‰ç´¢å¼•è¯
        index_terms = set()
        for keyword in (item.keywords or []):
            keyword = (keyword or "").strip().lower()
            if keyword:
                index_terms.add(keyword)
        
        category = (item.category or "").strip().lower()
        if category:
            index_terms.add(category)
        
        tokens = self._extract_tokens(item.question)
        for token in tokens[:10]:
            index_terms.add(token.lower())
        
        # æ›´æ–°ç´¢å¼•
        for term in index_terms:
            if remove:
                # åˆ é™¤
                if term in self._inverted_index and item.id in self._inverted_index[term]:
                    self._inverted_index[term].remove(item.id)
                    if not self._inverted_index[term]:
                        del self._inverted_index[term]
            else:
                # æ·»åŠ 
                if term not in self._inverted_index:
                    self._inverted_index[term] = []
                if item.id not in self._inverted_index[term]:
                    self._inverted_index[term].append(item.id)
    
    def _chunk_text(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ‡ç‰‡"""
        chunk_size = self.config.get("chunk_size", 500)
        chunk_overlap = self.config.get("chunk_overlap", 50)
        
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - chunk_overlap
        
        return chunks
    
    def _rewrite_query(self, query: str) -> str:
        """æŸ¥è¯¢æ”¹å†™ - çŸ­è¯­åœç”¨è¯è¿‡æ»¤ + åŒä¹‰è¯æ‰©å±•"""
        
        # ç”µå•†å®¢æœåœºæ™¯åœç”¨è¯ï¼ˆåªä¿ç•™å®Œæ•´çŸ­è¯­ï¼Œä¸ä½¿ç”¨å•å­—ç¬¦ï¼‰
        # æŒ‰é•¿åº¦ä»é•¿åˆ°çŸ­æ’åºï¼Œé¿å…çŸ­è¯è¯¯åˆ é•¿è¯çš„ä¸€éƒ¨åˆ†
        stop_phrases = [
            # é•¿çŸ­è¯­ä¼˜å…ˆ
            "è¯·é—®ä¸€ä¸‹", "æƒ³é—®ä¸€ä¸‹", "é—®ä¸€ä¸‹", "æƒ³çŸ¥é“", "æˆ‘æƒ³é—®", "æˆ‘æƒ³çŸ¥é“",
            "å¯ä¸å¯ä»¥", "èƒ½ä¸èƒ½", "æ€ä¹ˆæ ·", "å¥½ä¸å¥½",
            "å¸®æˆ‘çœ‹çœ‹", "å¸®æˆ‘æŸ¥æŸ¥", "å¸®æˆ‘é—®é—®",
            "éº»çƒ¦é—®ä¸€ä¸‹", "éº»çƒ¦å¸®æˆ‘",
            # å¸¸è§å¼€å¤´è¯­
            "ä½ å¥½", "æ‚¨å¥½", "è¯·é—®", "æˆ‘æƒ³", "å¸®æˆ‘", "éº»çƒ¦",
            # å¸¸è§ç»“å°¾è¯­
            "è°¢è°¢", "æ„Ÿè°¢", "å¥½çš„", "å¯ä»¥å—", "è¡Œå—", "å¥½å—",
            # è¯­æ°”è¯ï¼ˆæ”¾æœ€åï¼Œåªå¤„ç†å¥é¦–å¥å°¾çš„ï¼‰
        ]
        
        # ç”µå•†é¢†åŸŸåŒä¹‰è¯æ˜ å°„ï¼ˆç”¨æˆ·å¸¸ç”¨è¯ â†’ æ£€ç´¢å…³é”®è¯ï¼‰
        synonym_map = {
            # ä¿ƒé”€æ´»åŠ¨ç›¸å…³
            "ä¿ƒé”€æ´»åŠ¨": "ä¼˜æƒ æ´»åŠ¨", "ä¿ƒé”€": "ä¼˜æƒ æ´»åŠ¨", "æ´»åŠ¨": "ä¼˜æƒ æ´»åŠ¨",
            "æœ‰ä»€ä¹ˆæ´»åŠ¨": "ä¼˜æƒ æ´»åŠ¨", "ä»€ä¹ˆæ´»åŠ¨": "ä¼˜æƒ æ´»åŠ¨",
            "å‚åŠ æ´»åŠ¨": "ä¼˜æƒ æ´»åŠ¨", "å‚åŠ ": "å‚ä¸",
            # ä»·æ ¼ç›¸å…³
            "å¤šå°‘é’±": "ä»·æ ¼", "ä»€ä¹ˆä»·": "ä»·æ ¼", "ä»·ä½": "ä»·æ ¼", 
            "è´µä¸è´µ": "ä»·æ ¼", "ä¾¿å®œ": "ä¼˜æƒ ", "æ‰“æŠ˜": "æŠ˜æ‰£ä¼˜æƒ ",
            "ä¼˜æƒ åˆ¸": "ä¼˜æƒ åˆ¸", "æ»¡å‡": "æ»¡å‡æ´»åŠ¨", "çº¢åŒ…": "ä¼˜æƒ çº¢åŒ…",
            # ç‰©æµç›¸å…³
            "å‘è´§": "ç‰©æµé…é€", "å¿«é€’": "ç‰©æµ", "é€è´§": "é…é€",
            "åˆ°è´§": "é€è¾¾", "å‡ å¤©åˆ°": "é…é€æ—¶é—´", "å¤šä¹…åˆ°": "é…é€æ—¶é—´",
            "åŒ…é‚®": "å…è¿è´¹", "é‚®è´¹": "è¿è´¹", "è¿è´¹å¤šå°‘": "è¿è´¹",
            # å”®åç›¸å…³
            "é€€è´§": "é€€æ¢è´§", "æ¢è´§": "é€€æ¢è´§", "é€€æ¬¾": "é€€æ¬¾",
            "ä¿ä¿®": "è´¨ä¿", "å”®å": "å”®åæœåŠ¡", "ç»´ä¿®": "ç»´ä¿®",
            "åäº†": "æ•…éšœ", "ä¸èƒ½ç”¨": "æ•…éšœ", "è´¨é‡é—®é¢˜": "è´¨é‡",
            # å•†å“ç›¸å…³
            "æœ‰è´§å—": "åº“å­˜", "æœ‰æ²¡æœ‰è´§": "åº“å­˜", "ç¼ºè´§": "åº“å­˜",
            "å°ºç ": "å°ºå¯¸", "å¤§å°": "å°ºå¯¸", "é¢œè‰²": "é¢œè‰²",
            "æ¬¾å¼": "æ¬¾å¼", "å‹å·": "å‹å·", "è§„æ ¼": "è§„æ ¼",
            # æ”¯ä»˜ç›¸å…³
            "ä»˜æ¬¾": "æ”¯ä»˜", "æ€ä¹ˆä»˜": "æ”¯ä»˜æ–¹å¼", 
            "åˆ†æœŸ": "åˆ†æœŸä»˜æ¬¾", "èŠ±å‘—": "æ”¯ä»˜", "ä¿¡ç”¨å¡": "æ”¯ä»˜",
            # è®¢å•ç›¸å…³
            "è®¢å•": "è®¢å•", "æŸ¥å•": "è®¢å•æŸ¥è¯¢", "å–æ¶ˆè®¢å•": "å–æ¶ˆè®¢å•",
            "ä¿®æ”¹è®¢å•": "ä¿®æ”¹è®¢å•", "è®¢å•çŠ¶æ€": "è®¢å•æŸ¥è¯¢",
            # è´¦æˆ·ç›¸å…³
            "å¯†ç ": "å¯†ç ", "ç™»å½•": "ç™»å½•", "æ³¨å†Œ": "æ³¨å†Œ", "è´¦å·": "è´¦æˆ·"
        }
        
        # 1. ç§»é™¤åœç”¨çŸ­è¯­ï¼ˆæŒ‰é•¿åº¦ä»é•¿åˆ°çŸ­ï¼Œé¿å…è¯¯åˆ ï¼‰
        cleaned_query = query
        for phrase in stop_phrases:
            cleaned_query = cleaned_query.replace(phrase, " ")
        
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        cleaned_query = " ".join(cleaned_query.split()).strip()
        
        # å¦‚æœæ¸…ç†åä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œä¿ç•™åŸå§‹æŸ¥è¯¢çš„æ ¸å¿ƒéƒ¨åˆ†
        if len(cleaned_query) < 2:
            cleaned_query = query
        
        # 2. åŒä¹‰è¯æ‰©å±• - åœ¨åŸå§‹æŸ¥è¯¢ä¸­æŸ¥æ‰¾ï¼Œæ·»åŠ æ£€ç´¢å…³é”®è¯
        expanded_terms = []
        for user_term, search_term in synonym_map.items():
            if user_term in query:
                # é¿å…é‡å¤æ·»åŠ 
                if search_term not in cleaned_query and search_term not in expanded_terms:
                    expanded_terms.append(search_term)
        
        # 3. æ„å»ºæ”¹å†™åçš„æŸ¥è¯¢
        if expanded_terms:
            rewritten = f"{cleaned_query} {' '.join(expanded_terms)}"
        else:
            rewritten = cleaned_query
        
        return rewritten

    def _merge_results(self, result_sets: List[List[Tuple[KnowledgeItem, float]]], limit: int) -> List[Tuple[KnowledgeItem, float]]:
        best: Dict[str, Tuple[KnowledgeItem, float]] = {}
        for results in result_sets:
            for item, score in results:
                prev = best.get(item.id)
                if prev is None or score > prev[1]:
                    best[item.id] = (item, score)

        merged = list(best.values())
        merged.sort(key=lambda x: x[1], reverse=True)
        return merged[:limit]

    def _compute_confidence(self, query: str, results: List[Tuple[KnowledgeItem, float]]) -> float:
        if not results:
            return 0.0

        top1 = results[0][1]
        top2 = results[1][1] if len(results) >= 2 else None
        gap = (top1 - top2) if top2 is not None else 0.0

        item = results[0][0]
        keywords = [k for k in (item.keywords or []) if k]
        if keywords:
            denom = min(len(keywords), 6)
            hit = sum(1 for kw in keywords[:denom] if kw in query)
            keyword_cover = hit / max(1, denom)
        else:
            keyword_cover = 0.0

        top_k_bonus = (min(len(results), 5) - 1) / 4 if len(results) >= 2 else 0.0

        confidence = top1
        confidence += 0.15 * max(0.0, min(gap, 1.0))
        confidence += 0.08 * max(0.0, min(keyword_cover, 1.0))
        confidence += 0.04 * max(0.0, min(top_k_bonus, 1.0))

        if confidence < 0.0:
            confidence = 0.0
        if confidence > 1.0:
            confidence = 1.0
        return confidence

    def _average_vectors(self, vectors: List[List[float]]) -> Optional[List[float]]:
        if not vectors:
            return None
        dim = len(vectors[0])
        acc = [0.0] * dim
        for vec in vectors:
            if not vec or len(vec) != dim:
                return None
            for i, v in enumerate(vec):
                acc[i] += v
        n = float(len(vectors))
        return [v / n for v in acc]

    def _item_base_text(self, item: "KnowledgeItem") -> str:
        return f"{item.question} {item.answer}".strip()

    def _make_chunk_id(self, item_id: str, chunk_idx: int) -> str:
        return f"{item_id}#chunk_{int(chunk_idx)}"

    def _split_chunk_id(self, stored_id: str) -> Tuple[Optional[str], Optional[int]]:
        if not stored_id:
            return None, None
        if "#" not in stored_id:
            return stored_id, None
        base, rest = stored_id.split("#", 1)
        base = (base or "").strip()
        if not base:
            return None, None
        m = re.search(r"(\d+)", rest or "")
        if not m:
            return base, None
        try:
            return base, int(m.group(1))
        except Exception:
            return base, None

    def _extract_tokens(self, text: str) -> List[str]:
        s = (text or "").strip()
        if not s:
            return []

        tokens: List[str] = []
        s_lower = s.lower()
        tokens.extend(re.findall(r"[a-z0-9]{2,}", s_lower))
        for seg in re.findall(r"[\u4e00-\u9fff]{2,}", s):
            seg = seg.strip()
            if not seg:
                continue
            tokens.append(seg)
            remain = 12
            for i in range(len(seg) - 1):
                if remain <= 0:
                    break
                tokens.append(seg[i : i + 2])
                remain -= 1

        seen = set()
        uniq: List[str] = []
        for t in tokens:
            if t and t not in seen:
                seen.add(t)
                uniq.append(t)
            if len(uniq) >= 40:
                break
        return uniq

    def _keyword_coverage_score(self, query: str, item: "KnowledgeItem", chunk_texts: Optional[List[str]] = None) -> float:
        tokens = self._extract_tokens(query)
        if not tokens:
            return 0.0

        q = (query or "").strip()
        base_text = self._item_base_text(item)
        pool = (chunk_texts or []) + [item.question or "", item.answer or "", base_text]

        hits = 0
        for t in tokens:
            if any(t in p for p in pool if p):
                hits += 1

        cover = hits / max(1, len(tokens))

        kw_hits = 0
        for kw in (item.keywords or []):
            if kw and kw in q:
                kw_hits += 1
        kw_bonus = min(1.0, 0.4 * kw_hits)

        return min(1.0, 0.75 * cover + 0.25 * kw_bonus)

    def _vector_search_multi(self, queries: List[str], threshold: float) -> List[Tuple[KnowledgeItem, float]]:
        embedding_client = self._get_embedding_client()
        vector_store = self._get_vector_store()
        if not embedding_client or not vector_store:
            return []
        if not embedding_client.is_available():
            logger.warning("EmbeddingæœåŠ¡ä¸å¯ç”¨ï¼Œé™çº§åˆ°å…³é”®è¯åŒ¹é…")
            return []

        uniq = []
        for q in queries:
            q = (q or "").strip()
            if q and q not in uniq:
                uniq.append(q)
        if not uniq:
            return []

        vecs = embedding_client.embed_texts(uniq)
        if not vecs:
            return []

        top_k = self.config.get("retrieval_top_k", 5)
        best: Dict[str, Tuple[KnowledgeItem, float]] = {}
        best_chunks: Dict[str, List[str]] = {}

        for q, vec in zip(uniq, vecs):
            results, chunk_map = self._vector_search_vec_detailed(q, vec, threshold)
            for item, score in results:
                prev = best.get(item.id)
                if prev is None or score > prev[1]:
                    best[item.id] = (item, score)
                    parts = chunk_map.get(item.id)
                    if parts:
                        best_chunks[item.id] = parts

        merged = list(best.values())
        merged.sort(key=lambda x: x[1], reverse=True)
        merged = merged[:top_k]
        self._last_chunk_map = best_chunks
        return merged

    def _keyword_search_multi(self, queries: List[str], threshold: float) -> List[Tuple[KnowledgeItem, float]]:
        uniq = []
        for q in queries:
            q = (q or "").strip()
            if q and q not in uniq:
                uniq.append(q)
        if not uniq:
            return []

        top_k = self.config.get("retrieval_top_k", 5)
        result_sets = [self._keyword_search(q, threshold) for q in uniq]
        return self._merge_results(result_sets, top_k)
    
    def search(self, query: str, threshold: float = None) -> List[Tuple[KnowledgeItem, float]]:
        """æœç´¢çŸ¥è¯†åº“ï¼Œä¼˜å…ˆä½¿ç”¨å‘é‡æ£€ç´¢ï¼Œå¤±è´¥æ—¶é™çº§åˆ°å…³é”®è¯åŒ¹é…"""
        import time as time_module
        start_time = time_module.perf_counter()
        search_method = "keyword"
        
        if threshold is None:
            threshold = self.config.get("similarity_threshold", 0.4)
        
        # åˆå§‹åŒ–æœç´¢ç»“æœ
        self._last_search_result = RAGSearchResult()
        self._last_chunk_map = {}
        self._last_search_result.query = query
        rewritten_query = self._rewrite_query(query)
        self._last_search_result.rewritten_query = rewritten_query

        queries = [rewritten_query, query]
        
        results = self._vector_search_multi(queries, threshold)
        
        if results:
            self._last_search_result.search_method = "vector"
            search_method = "vector"
        else:
            results = self._keyword_search_multi(queries, threshold)
            self._last_search_result.search_method = "keyword"
            search_method = "keyword"
        
        self._last_search_result.retrieved_items = results
        
        # æ„å»ºä¸Šä¸‹æ–‡
        if results:
            context_parts: List[str] = []
            max_context_chars = int(self.config.get("context_max_chars", 4000) or 4000)
            context_top_n = int(self.config.get("context_top_n", 3) or 3)

            total = 0
            chunk_map = self._last_chunk_map or {}

            for item, _ in results[:max(1, context_top_n)]:
                parts = chunk_map.get(item.id)
                if not parts:
                    parts = [f"é—®é¢˜ï¼š{item.question}\nç­”æ¡ˆï¼š{item.answer}"]
                for p in parts:
                    p = (p or "").strip()
                    if not p:
                        continue
                    next_total = total + len(p)
                    if max_context_chars > 0 and next_total > max_context_chars:
                        if not context_parts:
                            context_parts.append(truncate_text(p, max_context_chars))
                        total = max_context_chars
                        break
                    context_parts.append(p)
                    total = next_total
                if max_context_chars > 0 and total >= max_context_chars:
                    break

            self._last_search_result.context_text = "\n\n---\n\n".join(context_parts)
            self._last_search_result.confidence = self._compute_confidence(query, results)
        else:
            self._last_search_result.confidence = 0.0

        max_context_chars = self.config.get("context_max_chars", 4000)
        system_prompt = build_system_prompt(
            truncate_text(self._last_search_result.context_text, max_context_chars) if self._last_search_result.context_text else None
        )
        messages = build_messages(system_prompt, query)
        self._last_search_result.final_prompt = format_prompt_preview(messages)
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        duration = time_module.perf_counter() - start_time
        perf_monitor = self._get_perf_monitor()
        if perf_monitor:
            metric_name = "vector_search" if search_method == "vector" else "keyword_search"
            perf_monitor.record(metric_name, duration, True, {
                "query_length": len(query),
                "results_count": len(results)
            })
        
        return results

    def _vector_search_vec_detailed(
        self,
        query: str,
        query_vec: List[float],
        threshold: float,
    ) -> Tuple[List[Tuple[KnowledgeItem, float]], Dict[str, List[str]]]:
        vector_store = self._get_vector_store()
        if not vector_store or not query_vec:
            return [], {}

        top_k = int(self.config.get("retrieval_top_k", 5) or 5)
        candidate_k = max(top_k * 3, top_k)
        raw = vector_store.search(query_vec, candidate_k)

        last_error = getattr(vector_store, "last_error", None)
        if last_error and isinstance(last_error, dict) and last_error.get("type") == "dimension_mismatch":
            expected = last_error.get("expected")
            actual = last_error.get("actual")
            logger.warning(
                "å‘é‡ç´¢å¼•ç»´åº¦ä¸åŒ¹é…ï¼Œå·²è‡ªåŠ¨ç¦ç”¨å‘é‡æ£€ç´¢: ç´¢å¼•ç»´åº¦%s, å‘é‡ç»´åº¦%s",
                expected,
                actual,
            )
            return [], {}

        chunk_top_n = int(self.config.get("chunk_top_n", 2) or 2)

        hits: Dict[str, dict] = {}
        for stored_id, score in raw:
            if score < threshold:
                continue
            item_id, chunk_idx = self._split_chunk_id(stored_id)
            if not item_id:
                continue
            item = self.get_item_by_id(item_id)
            if not item:
                continue

            h = hits.get(item_id)
            if h is None:
                h = {"item": item, "max": float(score), "chunks": {}}
                hits[item_id] = h
            else:
                if float(score) > float(h.get("max", 0.0)):
                    h["max"] = float(score)

            base_text = self._item_base_text(item)
            if chunk_idx is None:
                chunk_text = base_text
            else:
                parts = self._chunk_text(base_text)
                if 0 <= chunk_idx < len(parts):
                    chunk_text = parts[chunk_idx]
                else:
                    chunk_text = base_text

            chunks: Dict[str, float] = h["chunks"]
            prev = chunks.get(chunk_text)
            if prev is None or float(score) > float(prev):
                chunks[chunk_text] = float(score)

        if not hits:
            return [], {}

        ranked: List[Tuple[KnowledgeItem, float]] = []
        chunk_map: Dict[str, List[str]] = {}
        for item_id, h in hits.items():
            item = h["item"]
            max_score = float(h.get("max", 0.0))
            chunks: Dict[str, float] = h.get("chunks") or {}
            best_chunks = sorted(chunks.items(), key=lambda x: x[1], reverse=True)[: max(1, chunk_top_n)]
            chunk_texts = [t for t, _ in best_chunks if t]

            cover = self._keyword_coverage_score(query, item, chunk_texts)
            bonus = min(0.25, 0.25 * cover)
            final_score = max_score + bonus
            if final_score > 1.0:
                final_score = 1.0

            ranked.append((item, final_score))

            formatted: List[str] = []
            for t in chunk_texts[: max(1, chunk_top_n)]:
                formatted.append(f"é—®é¢˜ï¼š{item.question}\nå†…å®¹ï¼š{t}")
            if formatted:
                chunk_map[item_id] = formatted

        ranked.sort(key=lambda x: x[1], reverse=True)
        return ranked[:top_k], chunk_map
    
    def _vector_search(self, query: str, threshold: float) -> List[Tuple[KnowledgeItem, float]]:
        """å‘é‡æ£€ç´¢"""
        embedding_client = self._get_embedding_client()
        vector_store = self._get_vector_store()
        
        if not embedding_client or not vector_store:
            return []
        
        if not embedding_client.is_available():
            logger.warning("EmbeddingæœåŠ¡ä¸å¯ç”¨ï¼Œé™çº§åˆ°å…³é”®è¯åŒ¹é…")
            return []
        
        # å‘é‡åŒ–æŸ¥è¯¢
        query_vec = embedding_client.embed_text(query)
        if not query_vec:
            return []
        
        # æœç´¢
        top_k = self.config.get("retrieval_top_k", 5)
        search_results = vector_store.search(query_vec, top_k)

        last_error = getattr(vector_store, "last_error", None)
        if last_error and isinstance(last_error, dict) and last_error.get("type") == "dimension_mismatch":
            expected = last_error.get("expected")
            actual = last_error.get("actual")
            logger.warning(
                "å‘é‡ç´¢å¼•ç»´åº¦ä¸åŒ¹é…ï¼Œå·²è‡ªåŠ¨ç¦ç”¨å‘é‡æ£€ç´¢: ç´¢å¼•ç»´åº¦%s, å‘é‡ç»´åº¦%s",
                expected,
                actual,
            )
            return []
        
        results = []
        for item_id, score in search_results:
            if score >= threshold:
                # æ‰¾åˆ°å¯¹åº”çš„çŸ¥è¯†æ¡ç›®
                item = self.get_item_by_id(item_id)
                if item:
                    results.append((item, score))
        
        return results
    
    def _keyword_search(self, query: str, threshold: float) -> List[Tuple[KnowledgeItem, float]]:
        """å…³é”®è¯åŒ¹é…ï¼ˆä½¿ç”¨å€’æ’ç´¢å¼•åŠ é€Ÿï¼‰"""
        results: List[Tuple[KnowledgeItem, float]] = []
        q = (query or "").strip()
        if not q:
            return []

        q_lower = q.lower()
        tokens = self._extract_tokens(q)
        if not tokens:
            tokens = [q]

        top_k = int(self.config.get("retrieval_top_k", 5) or 5)
        
        # ä½¿ç”¨å€’æ’ç´¢å¼•å¿«é€Ÿè·å–å€™é€‰é›†
        candidate_ids = set()
        if self._index_built and self._inverted_index:
            for token in tokens:
                token_lower = token.lower()
                if token_lower in self._inverted_index:
                    candidate_ids.update(self._inverted_index[token_lower])
            
            # å¦‚æœå€’æ’ç´¢å¼•æ²¡æœ‰å‘½ä¸­ï¼Œå›é€€åˆ°å…¨é‡æœç´¢
            if not candidate_ids:
                candidate_ids = {item.id for item in self.items}
        else:
            candidate_ids = {item.id for item in self.items}
        
        # åªå¯¹å€™é€‰é›†è¿›è¡Œè¯¦ç»†è¯„åˆ†
        for item in self.items:
            if item.id not in candidate_ids:
                continue
                
            score = 0.0

            kw_hits = 0
            for keyword in (item.keywords or []):
                if keyword and keyword in q:
                    kw_hits += 1
            if kw_hits:
                score += min(0.7, 0.35 * kw_hits)

            q_hit = 0
            a_hit = 0
            for t in tokens:
                if t and item.question and t in item.question:
                    q_hit += 1
                if t and item.answer and t in item.answer:
                    a_hit += 1

            denom = max(1, len(tokens))
            score += 0.22 * (q_hit / denom)
            score += 0.14 * (a_hit / denom)

            if item.question and q_lower in item.question.lower():
                score += 0.12
            if item.answer and q_lower in item.answer.lower():
                score += 0.08

            if score >= threshold:
                if score > 1.0:
                    score = 1.0
                results.append((item, score))

        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def get_item_by_id(self, item_id: str) -> Optional[KnowledgeItem]:
        """æ ¹æ®IDè·å–çŸ¥è¯†æ¡ç›®"""
        for item in self.items:
            if item.id == item_id:
                return item
        return None
    
    def get_last_search_result(self) -> Optional[RAGSearchResult]:
        """è·å–æœ€è¿‘ä¸€æ¬¡æœç´¢çš„è¯¦ç»†ç»“æœï¼ˆç”¨äºRAGè¿½æº¯ï¼‰"""
        return self._last_search_result
    
    def check_duplicate(self, question: str, threshold: float = 0.85) -> Optional[Tuple[KnowledgeItem, float]]:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡å¤æˆ–ç›¸ä¼¼çš„çŸ¥è¯†æ¡ç›®
        
        Args:
            question: è¦æ£€æŸ¥çš„é—®é¢˜
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        
        Returns:
            å¦‚æœå­˜åœ¨ç›¸ä¼¼æ¡ç›®ï¼Œè¿”å› (æ¡ç›®, ç›¸ä¼¼åº¦)ï¼Œå¦åˆ™è¿”å› None
        """
        if not question or not question.strip():
            return None
        
        question = question.strip().lower()
        
        # 1. ç²¾ç¡®åŒ¹é…æ£€æŸ¥
        for item in self.items:
            if item.question.strip().lower() == question:
                return (item, 1.0)
        
        # 2. ç®€å•ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆåŸºäºå­—ç¬¦é‡å ï¼‰
        def simple_similarity(s1: str, s2: str) -> float:
            """è®¡ç®—ç®€å•çš„å­—ç¬¦çº§ç›¸ä¼¼åº¦"""
            s1, s2 = s1.lower(), s2.lower()
            if not s1 or not s2:
                return 0.0
            
            # ä½¿ç”¨å­—ç¬¦é›†åˆçš„Jaccardç›¸ä¼¼åº¦
            set1 = set(s1)
            set2 = set(s2)
            intersection = len(set1 & set2)
            union = len(set1 | set2)
            
            if union == 0:
                return 0.0
            
            return intersection / union
        
        best_match = None
        best_score = 0.0
        
        for item in self.items:
            score = simple_similarity(question, item.question)
            if score > best_score:
                best_score = score
                best_match = item
        
        if best_match and best_score >= threshold:
            return (best_match, best_score)
        
        # 3. å¦‚æœå‘é‡æ£€ç´¢å¯ç”¨ï¼Œä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦
        embedding_client = self._get_embedding_client()
        vector_store = self._get_vector_store()
        
        if embedding_client and vector_store and embedding_client.is_available():
            try:
                query_vec = embedding_client.embed_text(question)
                if query_vec:
                    results = vector_store.search(query_vec, top_k=1)
                    if results:
                        item_id, score = results[0]
                        if score >= threshold:
                            item = self.get_item_by_id(item_id.split("#")[0])  # å¤„ç†chunk ID
                            if item:
                                return (item, score)
            except Exception as e:
                logger.debug("å‘é‡ç›¸ä¼¼åº¦æ£€æŸ¥å¤±è´¥: %s", e)
        
        return None
    
    def add_item(self, question: str, answer: str, keywords: List[str], category: str = "é€šç”¨") -> KnowledgeItem:
        """æ·»åŠ çŸ¥è¯†æ¡ç›®"""
        # æ€§èƒ½ç›‘æ§
        perf = self._get_perf_monitor()
        if perf:
            perf.record("knowledge_add", 0.0, True)
        
        # ç”Ÿæˆæ–°ID
        max_id = 0
        for item in self.items:
            try:
                num = int(item.id[1:])
                max_id = max(max_id, num)
            except:
                pass
        item_id = f"K{max_id + 1:03d}"
        
        item = KnowledgeItem(
            id=item_id,
            question=question,
            answer=answer,
            keywords=keywords,
            category=category
        )
        self.items.append(item)
        self._save_to_file()
        
        # æ›´æ–°å€’æ’ç´¢å¼•
        self._update_inverted_index(item, remove=False)
        
        # åŒæ­¥æ›´æ–°å‘é‡ç´¢å¼•
        self._add_to_vector_index(item)
        
        return item
    
    def _add_to_vector_index(self, item: KnowledgeItem):
        """å°†çŸ¥è¯†æ¡ç›®æ·»åŠ åˆ°å‘é‡ç´¢å¼•"""
        self._last_vector_index_error = None
        embedding_client = self._get_embedding_client()
        vector_store = self._get_vector_store()
        
        if not embedding_client or not vector_store:
            return
        
        if not embedding_client.is_available():
            return
        
        try:
            vector_store.remove_vector(item.id)
            vector_store.remove_vectors_by_prefix(f"{item.id}#")
        except Exception:
            pass

        text = self._item_base_text(item)
        chunks = self._chunk_text(text)
        max_chunks = int(self.config.get("chunk_max_per_item", 6) or 6)
        chunks = [c for c in (chunks[:max(1, max_chunks)] if chunks else [text]) if c]
        if not chunks:
            return

        vecs = embedding_client.embed_texts(chunks)
        if not vecs or len(vecs) != len(chunks):
            return

        for i, vec in enumerate(vecs):
            if not vec:
                continue
            cid = self._make_chunk_id(item.id, i)
            ok = vector_store.add_vector(cid, vec)
            if not ok:
                last_error = getattr(vector_store, "last_error", None)
                if isinstance(last_error, dict):
                    self._last_vector_index_error = last_error
                logger.warning("å‘é‡ç´¢å¼•æœªæ›´æ–°: %s", item.id)
                return

        vector_store.save()
        logger.info("å·²æ·»åŠ å‘é‡ç´¢å¼•: %s", item.id)
    
    def delete_item(self, item_id: str) -> bool:
        """åˆ é™¤çŸ¥è¯†æ¡ç›®"""
        for i, item in enumerate(self.items):
            if item.id == item_id:
                # å…ˆæ›´æ–°å€’æ’ç´¢å¼•
                self._update_inverted_index(item, remove=True)
                
                del self.items[i]
                self._save_to_file()
                
                # åŒæ­¥åˆ é™¤å‘é‡ç´¢å¼•
                vector_store = self._get_vector_store()
                if vector_store:
                    vector_store.remove_vector(item_id)
                    vector_store.remove_vectors_by_prefix(f"{item_id}#")
                    vector_store.save()
                
                return True
        return False
    
    def update_item(self, item_id: str, **kwargs) -> bool:
        """æ›´æ–°çŸ¥è¯†æ¡ç›®"""
        for item in self.items:
            if item.id == item_id:
                # å…ˆä»å€’æ’ç´¢å¼•ä¸­ç§»é™¤æ—§æ•°æ®
                self._update_inverted_index(item, remove=True)
                
                for key, value in kwargs.items():
                    if hasattr(item, key):
                        setattr(item, key, value)
                self._save_to_file()
                
                # æ·»åŠ æ–°æ•°æ®åˆ°å€’æ’ç´¢å¼•
                self._update_inverted_index(item, remove=False)
                
                # é‡æ–°ç´¢å¼•å‘é‡
                self._add_to_vector_index(item)
                
                return True
        return False
    
    def rebuild_vector_index(self, progress_callback: Callable[[str, int, int], None] = None) -> Tuple[bool, str]:
        """é‡å»ºå‘é‡ç´¢å¼•"""
        self._last_vector_index_error = None
        embedding_client = self._get_embedding_client()
        vector_store = self._get_vector_store()
        
        if not embedding_client:
            return False, "Embeddingå®¢æˆ·ç«¯ä¸å¯ç”¨"
        
        if not vector_store:
            return False, "å‘é‡å­˜å‚¨ä¸å¯ç”¨"
        
        if not embedding_client.is_available():
            return False, "è¯·å…ˆé…ç½®APIå¯†é’¥"
        
        # æ¸…ç©ºç´¢å¼•
        if progress_callback:
            progress_callback("æ¸…ç©ºç´¢å¼•", 0, 1)
        vector_store.clear()
        
        max_chunks = int(self.config.get("chunk_max_per_item", 6) or 6)

        chunk_texts: List[str] = []
        chunk_ids: List[str] = []
        for item in self.items:
            text = self._item_base_text(item)
            chunks = self._chunk_text(text)
            chunks = [c for c in (chunks[:max(1, max_chunks)] if chunks else [text]) if c]
            for i, c in enumerate(chunks):
                chunk_texts.append(c)
                chunk_ids.append(self._make_chunk_id(item.id, i))

        if progress_callback:
            progress_callback("å‘é‡åŒ–", 0, max(len(chunk_texts), 1))
        vectors = embedding_client.embed_texts(chunk_texts)
        if not vectors or len(vectors) != len(chunk_texts):
            return False, "å‘é‡åŒ–å¤±è´¥"

        wrote = 0
        if progress_callback:
            progress_callback("å†™å…¥ç´¢å¼•", 0, max(len(chunk_texts), 1))
        for i, (cid, vec) in enumerate(zip(chunk_ids, vectors)):
            if vec:
                ok = vector_store.add_vector(cid, vec)
                if not ok:
                    self._last_vector_index_error = getattr(vector_store, "last_error", None)
                    return False, "å†™å…¥ç´¢å¼•å¤±è´¥ï¼Œè¯·æ£€æŸ¥Embeddingæ¨¡å‹å¹¶é‡å»ºç´¢å¼•"
                wrote += 1
            if progress_callback:
                progress_callback("å†™å…¥ç´¢å¼•", i + 1, max(len(chunk_texts), 1))

        vector_store.save()
        return True, f"æˆåŠŸç´¢å¼• {len(self.items)} æ¡çŸ¥è¯†ï¼ˆ{wrote} ä¸ªchunkå‘é‡ï¼‰"
    
    def get_all_items(self) -> List[KnowledgeItem]:
        """è·å–æ‰€æœ‰æ¡ç›®"""
        return self.items.copy()
    
    def get_categories(self) -> List[str]:
        """è·å–æ‰€æœ‰åˆ†ç±»"""
        return list(set(item.category for item in self.items))
    
    def reload(self):
        """é‡æ–°åŠ è½½çŸ¥è¯†åº“"""
        self._load_from_file()
        # é‡å»ºå€’æ’ç´¢å¼•
        self._build_inverted_index()


class ProductStore:
    """å•†å“å­˜å‚¨ - JSONæ–‡ä»¶æŒä¹…åŒ– + çŸ¥è¯†åº“åŒæ­¥"""
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        self._initialized = True
        self.products: List[ProductItem] = []
        self._data_file = self._get_data_file()
        self._knowledge_store = None
        self._load_from_file()
    
    def _get_knowledge_store(self):
        """å»¶è¿ŸåŠ è½½çŸ¥è¯†åº“"""
        if self._knowledge_store is None:
            self._knowledge_store = KnowledgeStore()
        return self._knowledge_store
    
    def _get_data_file(self) -> str:
        """è·å–æ•°æ®æ–‡ä»¶è·¯å¾„"""
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        data_dir = os.path.join(base_dir, "data")
        os.makedirs(data_dir, exist_ok=True)
        return os.path.join(data_dir, "products.json")
    
    def _load_from_file(self):
        """ä»JSONæ–‡ä»¶åŠ è½½å•†å“"""
        if os.path.exists(self._data_file):
            try:
                with open(self._data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.products = [ProductItem.from_dict(item) for item in data.get("products", [])]
                    logger.info("å·²åŠ è½½ %s ä¸ªå•†å“", len(self.products))
            except Exception as e:
                logger.exception("åŠ è½½å•†å“æ•°æ®å¤±è´¥")
                self.products = []
        else:
            self.products = []
            self._save_to_file()
    
    def _save_to_file(self):
        """ä¿å­˜å•†å“åˆ°JSONæ–‡ä»¶ï¼ˆå¸¦æ–‡ä»¶é”ï¼‰"""
        try:
            from core.file_lock import FileLock
            
            data = {
                "products": [product.to_dict() for product in self.products],
                "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            # ä½¿ç”¨æ–‡ä»¶é”ä¿æŠ¤å†™å…¥
            lock = FileLock(self._data_file, timeout=5.0)
            with lock:
                with open(self._data_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info("å•†å“æ•°æ®å·²ä¿å­˜ï¼Œå…± %s ä¸ª", len(self.products))
        except TimeoutError:
            logger.error("ä¿å­˜å•†å“æ•°æ®å¤±è´¥ï¼šæ— æ³•è·å–æ–‡ä»¶é”")
        except Exception as e:
            logger.exception("ä¿å­˜å•†å“æ•°æ®å¤±è´¥")
    
    def _get_product_knowledge_ids(self, product_id: str) -> List[str]:
        """è·å–å•†å“å¯¹åº”çš„çŸ¥è¯†æ¡ç›®IDåˆ—è¡¨"""
        return [f"{product_id}_K{i}" for i in range(1, 5)]  # æœ€å¤š4ä¸ªçŸ¥è¯†æ¡ç›®
    
    def add_product(self, name: str, price: float, category: str, description: str,
                    specifications: Dict[str, str] = None, stock: int = 0,
                    keywords: List[str] = None) -> ProductItem:
        """æ·»åŠ å•†å“"""
        # ç”Ÿæˆæ–°ID
        max_id = 0
        for product in self.products:
            try:
                num = int(product.id[1:])
                max_id = max(max_id, num)
            except:
                pass
        product_id = f"P{max_id + 1:03d}"
        
        product = ProductItem(
            id=product_id,
            name=name,
            price=price,
            category=category,
            description=description,
            specifications=specifications or {},
            stock=stock,
            keywords=keywords or []
        )
        self.products.append(product)
        self._save_to_file()
        
        # åŒæ­¥æ·»åŠ çŸ¥è¯†æ¡ç›®
        self._sync_product_to_knowledge(product)
        
        return product
    
    def _sync_product_to_knowledge(self, product: ProductItem):
        """å°†å•†å“ä¿¡æ¯åŒæ­¥åˆ°çŸ¥è¯†åº“"""
        knowledge_store = self._get_knowledge_store()
        knowledge_items = product.generate_knowledge_items()
        
        for i, item_data in enumerate(knowledge_items, 1):
            # ä½¿ç”¨ç‰¹æ®ŠIDæ ¼å¼ï¼šå•†å“ID_Kåºå·
            item_id = f"{product.id}_K{i}"
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›´æ–°
            existing = knowledge_store.get_item_by_id(item_id)
            if existing:
                knowledge_store.update_item(
                    item_id,
                    question=item_data["question"],
                    answer=item_data["answer"],
                    keywords=item_data["keywords"],
                    category=item_data["category"]
                )
            else:
                # æ‰‹åŠ¨åˆ›å»ºçŸ¥è¯†æ¡ç›®
                new_item = KnowledgeItem(
                    id=item_id,
                    question=item_data["question"],
                    answer=item_data["answer"],
                    keywords=item_data["keywords"],
                    category=item_data["category"]
                )
                knowledge_store.items.append(new_item)
                knowledge_store._save_to_file()
                knowledge_store._add_to_vector_index(new_item)

        logger.info("å·²åŒæ­¥å•†å“ %s çš„ %s æ¡çŸ¥è¯†", product.id, len(knowledge_items))
    
    def delete_product(self, product_id: str) -> bool:
        """åˆ é™¤å•†å“"""
        for i, product in enumerate(self.products):
            if product.id == product_id:
                del self.products[i]
                self._save_to_file()
                
                # åŒæ­¥åˆ é™¤çŸ¥è¯†æ¡ç›®
                self._remove_product_knowledge(product_id)
                
                return True
        return False
    
    def _remove_product_knowledge(self, product_id: str):
        """åˆ é™¤å•†å“å¯¹åº”çš„çŸ¥è¯†æ¡ç›®"""
        knowledge_store = self._get_knowledge_store()
        knowledge_ids = self._get_product_knowledge_ids(product_id)
        
        for kid in knowledge_ids:
            knowledge_store.delete_item(kid)

        logger.info("å·²åˆ é™¤å•†å“ %s çš„çŸ¥è¯†æ¡ç›®", product_id)
    
    def update_product(self, product_id: str, **kwargs) -> bool:
        """æ›´æ–°å•†å“"""
        for product in self.products:
            if product.id == product_id:
                for key, value in kwargs.items():
                    if hasattr(product, key):
                        setattr(product, key, value)
                self._save_to_file()
                
                # é‡æ–°åŒæ­¥çŸ¥è¯†æ¡ç›®
                self._remove_product_knowledge(product_id)
                self._sync_product_to_knowledge(product)
                
                return True
        return False
    
    def get_product_by_id(self, product_id: str) -> Optional[ProductItem]:
        """æ ¹æ®IDè·å–å•†å“"""
        for product in self.products:
            if product.id == product_id:
                return product
        return None
    
    def get_all_products(self) -> List[ProductItem]:
        """è·å–æ‰€æœ‰å•†å“"""
        return self.products.copy()
    
    def get_categories(self) -> List[str]:
        """è·å–æ‰€æœ‰å•†å“åˆ†ç±»"""
        return list(set(product.category for product in self.products))
    
    def search_products(self, query: str) -> List[ProductItem]:
        """æœç´¢å•†å“"""
        query_lower = query.lower()
        results = []
        for product in self.products:
            if (query_lower in product.name.lower() or
                query_lower in product.description.lower() or
                any(query_lower in kw.lower() for kw in product.keywords)):
                results.append(product)
        return results
    
    def reload(self):
        """é‡æ–°åŠ è½½å•†å“æ•°æ®"""
        self._load_from_file()
    
    def sync_all_to_knowledge(self) -> tuple:
        """å°†æ‰€æœ‰å•†å“åŒæ­¥åˆ°çŸ¥è¯†åº“"""
        success_count = 0
        fail_count = 0
        
        for product in self.products:
            try:
                self._sync_product_to_knowledge(product)
                success_count += 1
            except Exception as e:
                logger.exception("åŒæ­¥å•†å“ %s å¤±è´¥", product.id)
                fail_count += 1
        
        return success_count, fail_count
